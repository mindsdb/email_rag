import argparse
import logging

import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

_DEFAULT_THEME = 'dark'
_EVALUATION_METRICS = ['context_recall',
                       'context_precision', 'faithfulness', 'answer_relevancy']


def visualize_evaluation_metrics(title: str, evaluation_results: pd.DataFrame):
    '''Visualizes RAGAs library evaluation metrics using the seaborn library.

    Specifically, visualizes a histogram distribution for:
    - context_recall: The ability of the retriever to retrieve all the necessary information needed to answer the question.
    Conveys quality of the retrieval pipeline.
    - context_precision: A measure of how relevant the retrieved context is to the question.
    Conveys quality of the retrieval pipeline.
    - faithfulness: The factual consistency of the answer to the context base on the question.
    Conveys quality of the generation pipeline.
    - answer_relevancy: A measure of how relevant the answer is to the question.
    Conveys quality of the generation pipeline.

    Parameters:
        title (str): The title of the visualization to display
        evaluation_results (pd.DataFrame): A DataFrame containing the evaluation results (outputted by RAGAs)
    '''
    sns.set_theme(style=_DEFAULT_THEME)

    # There are 4 metrics so we use a 2x2 plot.
    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(
        nrows=2, ncols=2, figsize=(11, 8))
    f.suptitle(f'RAGAs Evaluation Metrics for {title}')
    sns.despine(f)

    for metric, axis in zip(_EVALUATION_METRICS, [ax1, ax2, ax3, ax4]):
        _ = sns.histplot(
            evaluation_results,
            x=metric,
            stat='percent',
            ax=axis,
        )
        axis.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())
        axis.set(ylim=(0, 100))
    plt.show()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        prog='Visualize RAG Evaluation Metrics',
        description='''Visualizes the RAGAs library evaluation metrics of a RAG pipeline.'''
    )
    parser.add_argument(
        '-p', '--path', help='Path of .csv evaluation results (generated by evaluate_rag.py) to visualize')
    parser.add_argument(
        '-l', '--log', help='Logging level to use (default WARNING)', default='WARNING')

    args = parser.parse_args()
    log_level = getattr(logging, args.log.upper())
    logging.basicConfig(level=log_level)

    dataset = pd.read_csv(args.path)
    visualize_evaluation_metrics(args.path, dataset)
